{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nj0T2cBwMbN_",
        "nrqiInXhMfYq"
      ],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
      ],
      "metadata": {
        "id": "awpRYgBQsscO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "UhwrEWgNMEPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Packages"
      ],
      "metadata": {
        "id": "h4xnU_xjMWl7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqK-a6HmL7Gb",
        "outputId": "1ceaadeb-53a2-4653-942d-105a04d7582c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import lightgbm as lgb\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook assumes that train.csv and test.csv are already downloaded"
      ],
      "metadata": {
        "id": "nj0T2cBwMbN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model & Create Submission"
      ],
      "metadata": {
        "id": "coxiOo3ZM7d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def process_data_improved(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Process data with high-cardinality categorical features\n",
        "    \"\"\"\n",
        "    # Base feature for values\n",
        "    base_feature = 'magical'\n",
        "\n",
        "    # Get consistent feature names\n",
        "    feature_names = ['magical','trickortreat_encoded', 'kingofhalloween_encoded']\n",
        "\n",
        "    # Calculate statistics from training data\n",
        "    base_median = train_df[base_feature].median()\n",
        "    Q1 = train_df[base_feature].quantile(0.25)\n",
        "    Q3 = train_df[base_feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Calculate robust target encodings for high-cardinality categorical variables\n",
        "    cat_encodings = {}\n",
        "    global_mean = train_df['y'].mean()\n",
        "\n",
        "    for col in ['trickortreat', 'kingofhalloween']:\n",
        "        # Group by category and calculate stats\n",
        "        cat_stats = (train_df.groupby(col)['y']\n",
        "                    .agg(['mean', 'count'])\n",
        "                    .reset_index())\n",
        "\n",
        "        # Only keep categories that appear more than once\n",
        "        frequent_cats = cat_stats[cat_stats['count'] > 1]\n",
        "\n",
        "        # Strong smoothing factor due to high cardinality\n",
        "        smoothing = 100\n",
        "\n",
        "        # Calculate smoothed means with stronger regularization\n",
        "        frequent_cats['encoded'] = (\n",
        "            (frequent_cats['count'] * frequent_cats['mean'] + smoothing * global_mean) /\n",
        "            (frequent_cats['count'] + smoothing)\n",
        "        )\n",
        "\n",
        "        # Create dictionary only for frequent categories\n",
        "        cat_encodings[col] = dict(zip(frequent_cats[col], frequent_cats['encoded']))\n",
        "\n",
        "    def process_single_df(df, is_train=True):\n",
        "        \"\"\"Process a single dataframe with high-cardinality handling\"\"\"\n",
        "        # Initialize output DataFrame\n",
        "        df_processed = pd.DataFrame(index=df.index, columns=feature_names)\n",
        "\n",
        "        # Process base feature\n",
        "        df_processed['magical'] = df['magical'].fillna(base_median).clip(lower_bound, upper_bound)\n",
        "\n",
        "        # Process categorical features\n",
        "        for col in ['trickortreat', 'kingofhalloween']:\n",
        "            # Map categories to encodings, with special handling for rare/unseen categories\n",
        "            df_processed[f'{col}_encoded'] = (\n",
        "                df[col].map(cat_encodings[col])\n",
        "                .fillna(global_mean)  # Use global mean for rare/unseen categories\n",
        "            )\n",
        "\n",
        "        # Add target if available\n",
        "        if 'y' in df.columns and is_train:\n",
        "            df_processed['y'] = df['y']\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    print(\"\\nProcessing training data...\")\n",
        "    train_processed = process_single_df(train_df, is_train=True)\n",
        "\n",
        "    print(\"\\nProcessing test data...\")\n",
        "    test_processed = process_single_df(test_df, is_train=False)\n",
        "\n",
        "    return train_processed, test_processed\n",
        "\n",
        "def create_improved_cat_model(train_df, test_df):\n",
        "    \"\"\"Create model with high-cardinality categorical features\"\"\"\n",
        "    # Process data\n",
        "    print(\"\\nProcessing data...\")\n",
        "    train_processed, test_processed = process_data_improved(train_df, test_df)\n",
        "\n",
        "    # Separate features and target\n",
        "    y = train_processed['y'].values\n",
        "    X = train_processed.drop('y', axis=1)\n",
        "\n",
        "    # Standardize numeric features\n",
        "    print(\"\\nStandardizing numeric features...\")\n",
        "    scaler = StandardScaler()\n",
        "    numeric_features = ['magical', 'trickortreat_encoded', 'kingofhalloween_encoded']\n",
        "\n",
        "    X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
        "    test_processed[numeric_features] = scaler.transform(test_processed[numeric_features])\n",
        "\n",
        "    print(f\"\\nFeatures being used: {X.columns.tolist()}\")\n",
        "    print(f\"Number of training samples: {len(X)}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create and train model with parameters adjusted for high cardinality\n",
        "    model = lgb.LGBMRegressor(\n",
        "        objective='rmse',\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=15,\n",
        "        random_state=42,\n",
        "        feature_fraction=0.7,\n",
        "        bagging_fraction=0.7,\n",
        "        bagging_freq=5,\n",
        "        min_child_samples=150,  # Increased to handle high cardinality\n",
        "        reg_alpha=0.2,          # Increased regularization\n",
        "        reg_lambda=0.2,         # Increased regularization\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining model...\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='rmse'\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinished Training model\")\n",
        "    print(\"\\nCreating Test Predictions\")\n",
        "\n",
        "    # Create test predictions\n",
        "    test_pred = model.predict(test_processed)\n",
        "\n",
        "    print(\"\\nFinished Creating Test Predictions\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'predictions': test_pred,\n",
        "        'processed_train': X,\n",
        "        'processed_test': test_processed,\n",
        "        'scaler': scaler\n",
        "    }\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "results = create_improved_cat_model(df, test_df)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'y': results['predictions']\n",
        "}).to_csv('submission.csv', index=False)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es_vX5ArM_Gq",
        "outputId": "119f10e3-2295-4156-9400-fb815befed78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing data...\n",
            "\n",
            "Processing training data...\n",
            "\n",
            "Processing test data...\n",
            "\n",
            "Standardizing numeric features...\n",
            "\n",
            "Features being used: ['magical', 'trickortreat_encoded', 'kingofhalloween_encoded']\n",
            "Number of training samples: 11000000\n",
            "\n",
            "Training model...\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102205 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 765\n",
            "[LightGBM] [Info] Number of data points in the train set: 8800000, number of used features: 3\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
            "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
            "[LightGBM] [Info] Start training from score 42348.078711\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[512]\tvalid_0's rmse: 620.809\n",
            "\n",
            "Finished Training model\n",
            "\n",
            "Creating Test Predictions\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
            "\n",
            "Finished Creating Test Predictions\n",
            "CPU times: user 7min 57s, sys: 6.83 s, total: 8min 4s\n",
            "Wall time: 1min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zP63kCIgOpRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
